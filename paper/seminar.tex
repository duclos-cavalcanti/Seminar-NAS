\documentclass[10pt,        % Don't change the font size!
               a4paper,     % Don't change the paper size!
               journal,     % Journal paper format
%               draft       % Enable this parameter to get a draft version.
               ]{IEEEtran}
\makeatletter

\def\markboth#1#2{\def\leftmark{\@IEEEcompsoconly{\sffamily}\MakeUppercase{\protect#1}}%
\def\rightmark{\@IEEEcompsoconly{\sffamily}\MakeUppercase{\protect#2}}}
\makeatother

% Packages
%\usepackage[latin1]{inputenc}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
% \usepackage[pdftex]{graphicx}
%\usepackage[cmex10]{amsmath}
%\usepackage{algorithmic}
%\usepackage{array}
%\usepackage{mdwmath}
%\usepackage{mdwtab}
%\usepackage{eqparbox}
%\usepackage[tight,footnotesize]{subfigure}
%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}

% \graphicspath{{../pdf/}{../jpeg/}}
% \DeclareGraphicsExtensions{.pdf,.jpeg,.png}

\begin{document}
%
% paper title
\title{Neural Architecture Search and tinyML}

\author{Daniel Duclos-Cavalcanti}

% The paper headers
\markboth{Seminar for VLSI Entwurfsverfahren, Summer Term 2022}%
{Daniel Duclos-Cavalcanti: Network Architecture Search (NAS)}

% make the title area
\maketitle

\begin{abstract}
There is no denying the increasing success that Deep Neural Networks (DNNs)
have displayed across various tasks such as image classification, speech recognition, machine translation
and many others. This progress can be attributed largely due to \textit{architecture engineering},
which is a process that requires immense domain expertise, intuition and some amount of trial
and error. This is not ideal as it is both time consuming and error-prone. Neural Architecture Search (NAS)
rises as the next logical step aiming to both automate architecture discovery and further the understanding
of the inner workings of DNNs. This field has grown remarkably within the last 5 years and among the many challenges
of NAS, there are concerns regarding computational costs and time feasability. This however changes within the context
of \textit{tinyML}, which is an expanding field at the intersection of machine learning and embedded systems.
Due to the resource constrained conditions involved in most embedded devices and tinyML
applications, the stages concerning training and performance evaluation are substantially faster in comparison to
the more complex models used in the research of NAS. Thus presenting an interesting opportunity to explore NAS applications
within a different paradigm. Throughout this work, an overview of existing research in NAS, specifically concerned
with the use of evolutionary algorithms and reinforcement learning will be presented, as well as highlighting
relevant applicabilities to tinyML.

\end{abstract}

\section{Introduction}
The advancement of deep learning, although extremely beneficial, has also caused a continuous demand for
architecture design. This coupled with a growing model complexity still requires ample time and expert knowledge to
continue this progress.

Finally, after the work in \cite{zoph2016neural} proposed by Google, it was shown for the first time that NAS algorithms
have the potential to find models that rival the current state of the art. However, done so in an automated fashion, minimizing
human participation. Since then, many different methods have appeared.

To better visualize NAS, one can categorize it within three dimensions \cite{wistuba2019survey}:
\begin{itemize}
    \item \textbf{Search Space}: defines all the possible architectures that can in principle be considered.

    \item \textbf{Search Strategy}: defines how the \textit{search space} is explored by the algorithm.

    \item \textbf{Performance Estimation Strategy}: defines how performance is evaluated at every architecture iteration.
\end{itemize}

\subsection{Search Space}
NAS is an optimization problem, whose search space is the defining factor to its complexity.
The smaller the search space is, the faster the search may converge,
as well as requiring less computational resources. This comes at the cost of less freedom to
explore unseen architectures and also possibly limiting the complexity of the design.

The simplest approach is to define an architecture as a \textit{chain-structured neural network}, which essentially consists of a
sequence of layers whose inputs are the output of their preceding layer. In this case the space is parametrized by maximum number
of layers, type of operations per layer, and hyperparameters conditioned by the chosen operation.

The next step consists then of including more modern design elements such as skip connections, which has already been seen
in \cite{zoph2016neural} and \cite{pmlr-v70-real17a}. This allows to build more complex \textit{multi-branch networks}, which
cannot be described as simple sequential layer chaining, but as a structure where each layer's input is a function of
previous layer outputs. Therefore, a chain-structured neural network can be seen as an edge-case or simplification of the multi-branch network.
This increases significantly the degrees of freedom of architecture design, which leads to a much larger search space.

Another predominant trend includes the search within \textit{cells} or \textit{blocks}, initially considered by works such as
\cite{zhong2018practical} and \cite{zoph2018learning}. What is proposed, is to break-off architectures in cells, such that
the search space is then designated within a single cell per time. This drastically reduces the search space, as there are
substantially less layers within a cell in comparison to an entire architecture. Additionally, subdivision of architectures
into blocks is considered a good design practice, which also enables easy transferability of blocks to other data sets. The same
modelling used on multi-branch networks can be used with cells, simply replacing layers with cell architectures.

This drives then the discussion between \textit{micro-architectures} versus \textit{macro-architectures}. The macro perspective
attempts to determine how cells should be connected and how many are needed to build a model. On the other hand, the micro
perspective aims to find the optimal structure for each cell. Ideally, both viewpoints should be optimized jointly, which of course
leads to a complex search space. There have been efforts that aimed to minimize this endeavor by fixing macro-architectures with
known working topographies such as in \cite{pmlr-v80-cai18a} with DenseNet \cite{Huang_2017_CVPR}. This practice, dubbed as
\textit{human injection} attempts to reduce the search space whilst using domain knowledge that is known to have effective
results. This can also be seen as adding human bias.

\subsection{Search Strategy}
Hello

\section{State of the Art}
In this section an analysis of the available literature on the topic is done.
This section may be split or subdivided into several sections or subsections.

\subsection{Subsection Heading Here}
Subsection text here.

% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol

\subsubsection{Subsubsection Heading Here}
Subsubsection text here.

\section{Conclusion}
Put the conclusions of the work here. The conclusion is like the abstract with
an additional discussion of open points.

% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% references section

\newpage
\bibliographystyle{IEEEtran}
\bibliography{refs}

%
\end{document}
