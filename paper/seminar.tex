\documentclass[10pt,        % Don't change the font size!
               a4paper,     % Don't change the paper size!
               journal,     % Journal paper format
%               draft       % Enable this parameter to get a draft version.
               ]{IEEEtran}
\makeatletter

\def\markboth#1#2{\def\leftmark{\@IEEEcompsoconly{\sffamily}\MakeUppercase{\protect#1}}%
\def\rightmark{\@IEEEcompsoconly{\sffamily}\MakeUppercase{\protect#2}}}
\makeatother

% Packages
%\usepackage[latin1]{inputenc}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{subcaption}

% \graphicspath{{../pdf/}{../jpeg/}}
% \DeclareGraphicsExtensions{.pdf,.jpeg,.png}

\begin{document}

% paper title
\title{Neural Architecture Search and tinyML: A Survey}
\author{Daniel~Duclos-Cavalcanti}

% The paper headers
\markboth{Seminar for VLSI Entwurfsverfahren, Summer Term 2022}%
{Daniel Duclos-Cavalcanti: Network Architecture Search (NAS)}

% make the title area
\maketitle

%\thanks{Daniel D-C. is with the Department
%of Electrical and Computer Engineering, Technical University of Munich, Bavaria, DE.}% <-this % stops a space

\begin{abstract}
There is no denying the increasing success that Deep Neural Networks (DNNs)
have displayed across various tasks such as image classification, speech recognition, machine translation
and many others. This progress can be attributed largely due to \textit{architecture engineering},
which is a process that requires immense domain expertise, intuition and some amount of trial
and error. This is not ideal as it is both time consuming and error-prone. Neural Architecture Search (NAS)
rises as the next logical step aiming to both automate architecture discovery and further the understanding
of the inner workings of DNNs. This field has grown remarkably within the last 5 years and among the many challenges
of NAS, there are concerns regarding computational costs and time feasability. This however changes within the context
of \textit{tinyML}, which is an expanding field at the intersection of machine learning and embedded systems.
Due to the resource constrained conditions involved in most embedded devices and tinyML
applications, the stages concerning training and performance evaluation are substantially faster in comparison to
the more complex models used in the research of NAS. Thus presenting an interesting opportunity to explore NAS applications
within a different paradigm. Throughout this work, an overview of existing research in NAS, specifically concerned
with the use of evolutionary algorithms methods will be presented, as well as highlighting relevant applicabilities to tinyML.

\end{abstract}

\begin{IEEEkeywords}
Neural Architecture Search (NAS), evolutionary computation (EC), Deep Learning, tinyML.
\end{IEEEkeywords}

\section{Introduction}
The advancement of deep learning, although extremely beneficial, has also caused a continuous demand for
architecture design. This coupled with a growing model complexity, demands ample time and expert knowledge for
any individual to not only benefit from it's application, but also be able to improve any given architecture.

After the work in \cite{zoph2016neural} proposed by Google, it was shown for the first time that NAS algorithms
have the potential to find models that rival the current state of the art. However, done so in an automated fashion, minimizing
human participation. Since then, many different methods have appeared.

To better visualize NAS and understand the difference between its different methods,
one can categorize it within three dimensions \cite{elsken2019neural}:
\begin{itemize}
    \item \textbf{Search Space}: defines all the possible architectures that can in principle be considered.

    \item \textbf{Search Strategy}: defines how the \textit{search space} is explored by the algorithm.

    \item \textbf{Performance Estimation Strategy}: defines how performance is evaluated at every architecture iteration.
\end{itemize}

\subsection{Search Space}
NAS is an optimization problem, whose search space is the defining factor to its complexity.
The smaller the search space is, the faster the search may converge,
as well as requiring less computational resources. This comes at the cost of less freedom to
explore unseen architectures and also possibly limiting the complexity of the design.

The simplest approach is to define an architecture as a \textit{chain-structured neural network}, which essentially consists of a
sequence of layers whose inputs are the output of their preceding layer. In this case the space is parametrized by maximum number
of layers, type of operations per layer, and hyperparameters conditioned by the chosen operation. An illustration to this can be
seen in Fig. \ref{space}.

The next step consists then of including more modern design elements such as skip connections, which has already been seen
in \cite{zoph2016neural} and \cite{pmlr-v70-real17a}. This allows to build more complex \textit{multi-branch networks}, which
cannot be described as simple sequential layer chaining, but as a structure where each layer's input is a function of
previous layers outputs. This increases significantly the degrees of freedom of architecture design,
which leads to a much larger search space.
An illustration to this can be seen in Fig. \ref{space}.
A chain-structured neural network is then a special case of the multi-branch network.

Another predominant trend includes the search within \textit{cells} or \textit{blocks}, initially considered by works such as
\cite{zhong2018practical} and \cite{zoph2018learning}. What is proposed, is to break-off architectures in cells, such that
the search space is then designated within a single cell per time. This drastically reduces the search space, as there are
substantially less layers within a cell in comparison to an entire architecture. Additionally, subdivision of architectures
into units is considered a good design practice, which also enables easy transferability to other data sets. The same
modelling used on multi-branch networks can be used with cells, simply replacing layers with cell architectures.

This drives then the discussion between \textit{micro-architectures} versus \textit{macro-architectures}. The macro architecture
attempts to determine how cells should be connected and how many are needed to build a model. On the other hand, the micro
architecture aims to find the optimal structure for each cell. Ideally, both viewpoints should be optimized jointly, which of course
leads to a complex search space. There have been efforts that aimed to minimize this endeavor by fixing macro-architectures with
known working topographies such as in \cite{pmlr-v80-cai18a} with DenseNet \cite{Huang_2017_CVPR}. This practice, dubbed as
\textit{human knowledge injection} attempts to reduce the search space through applying domain expertise known to obtain
effective results. This includes human bias in the model.

\begin{figure}[!t]
    \centering
    \includegraphics[scale=0.4]{space}
    \caption{Illustration of different ways to model neural architectures. Left: an example of a chain-structured NN.
             Right: an example of a multi-branch NN.}
    \label{space}
\end{figure}

% \begin{figure}
% \centering
%     \begin{subfigure}{0.1\textwidth}
%     \includegraphics[width=\textwidth]{example}
%     \caption{First subfigure.}
%     \label{fig:first}
% \end{subfigure}
% \hfill
%     \begin{subfigure}{0.1\textwidth}
%     \includegraphics[width=\textwidth]{multi}
%     \caption{Second subfigure.}
%     \label{fig:second}
% \end{subfigure}
% \caption{Creating subfigures in \LaTeX.}
% \label{fig:figures}
% \end{figure}

\subsection{Search Strategy}
As any space search problem, there is a \textit{exploration-exploitation} trade-off to be considered \cite{elsken2019neural}.
Obtaining a high-performing architecture within a feasible amount of time is desired. However, converging too early to a
suboptimal result is also not the goal.

Based on the current state of the art, NAS search algorithms can be classified mainly intro three different
categories \cite{liu2021survey}:

\subsubsection{\textbf{Reinforcement Learning} (RL)-based NAS Algorithms}
Here, one considers the development of a neural architecture the agent's action, where the action space is the same as
the search space. Therefore, it is then possible to frame NAS as a reinforcement learning problem \cite{elsken2019neural}.
After evaluating the performance of the given trained architecture on fitness data, it is possible to
determine the agent's reward. How this estimation is performed will depend on the given method, more to these
approaches will be seen at \ref{PES}. Furthermore, how the agent's policy is represented and it's optimization will
also vary. More on trade-offs and details to these approaches will be seen in \ref{SOA}.

\subsubsection{\textbf{Gradient}-based NAS Algorithms}
\textcolor{black}{
Consists of transforming the search space from discrete to continuous and performing
gradient descent with respect to the fitness data set. This transformation requires a set of conditions
and has still not been mathematically proven \cite{liu2021survey}.
}

\subsubsection{\textbf{Evolutionary Computation} (EC)-based NAS Algorithms}
By the application of well established EC methods, which are various techniques based on the evolution of
species within nature, many different efforts were implemented to navigate their respective search space.
\textcolor{black}{
Among others, genetic algorithms (GAs), genetic programming (GP) and particle swarm optimization (PSO)
techniques have already been successfully applied.
}
More on these different techniques will be seen in \ref{EV}.

\subsection{Performance Estimation Strategy}
\label{PES}
Independent of the search strategy, it is necessary to know how any given architecture performed in order
to guide the next steps of the algorithm. There are many ways to estimate said performance, whereas the simplest
would be complete training and validation. Given the complexity and size of the search spaces within NAS, this
requires GPU days in the order of thousands \cite{elsken2019neural}. This is why extensive research has been
employed to reduce time on performance estimation, since it is a significant time bottleneck.
Some of these approaches include \cite{elsken2019neural}:
\begin{itemize}
    \item \textbf{Lower Fidelity Techniques}: Shorter training time \cite{zela2018towards}, \cite{zoph2018learning}, training solely on a
        subset of the data \cite{klein2018towards}, training on downscaled data \cite{chrabaszcz2017downsampled} or with downscaled
        models \cite{zoph2018learning}, \cite{pmlr-v70-real17a}. These methods do introduce bias as performances will normally be
        underestimated.

    \item \textbf{Learning Curve Extrapolation}: Performance is extrapolated after just a small number of epochs and then
        decided upon directly. Klein et al. \cite{klein2018towards} considered architectural hyperparameters to predict which
        architectures are most promising after partial learning. Domhan et al \cite{domhan2015speeding} extrapolated partial
        learning curves to predict and eliminate sub-optimal architectures.

    \item \textbf{Weight Inheritance}: also dubbed as \textit{network morphisms} is a technique that passes down weights from
        previously trained models forward to new ones. This approach can cut down computational costs to just a few GPU days
        \cite{cai2018efficient}.

    \item \textbf{One-Shot Models}: also called \textit{weight sharing} is a technique that treats all architectures as subgraphs
        of a supergraph, which is named the one-shot model. Weights are shared between architectures that meet the condition of
        having edges in common. Finally, only the weights of a single one-shot model has to be trained and the
        sub-graph architectures can be evaluated directly as they inherit weights from the one-shot model. This demonstrates great
        success by cutting-down the entire process to a few GPU days. However, significant bias is introduced as the underestimation of architectures by this approach is harsh.
\end{itemize}

\section{State of the Art}
\label{SOA}

\subsection{Overview}
Currently RL-based algorithms are extremely costly in terms of computation, requiring thousands of
graphics processing cards (GPUs) for days. This is already the case for median-scale data sets, as
as data grows more complex, so does the processing time needed to search for a suitable architecture.

Gradient-based algorithms are faster, there are examples such as the DARTS algorithm, where processing power
is cut down to single digit GPUs \cite{liu2018darts}. However, due to the not completely compatible relationship
of NAS and a gradient-based optimization, frequently sub-optimal architectures are found \cite{liu2021survey}.

On the other hand, EC methods, while not perfect, have been around for decades and are easily applicable
to solve complex non-convex optimization problems, as they are insensitive to local minima and do not require
gradient information \cite{liu2021survey}.

\subsection{Evolutionary Neural Architecture Search (ENAS)}
\label{EV}

\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.4]{evol}
    \caption{Generic flowchart of a typical ENAS algorithm}
    \label{space}
\end{figure}

\subsection{tinyML and NAS}

\section{Conclusion}
Throughout this work a brief overview of the current research on NAS has been displayed. NAS is not only
a complex optimization problem, but also one that requires significant computational resources in most applications. The
trade-offs of the mainstream approaches to NAS have also been shown.

TinyML appears to be a promising field to explore NAS utilization, as performance estimation is extremely smaller within
this paradigm, being able to fully train and validate a model within minutes. However, other challenges are found such as
the need to train models not only considering performance, but also subgoals such as latency and parameter size, which
translates respectively to speed and memory on an embedded device.

% references section
\newpage
\bibliographystyle{IEEEtran}
\bibliography{refs}

%
\end{document}
