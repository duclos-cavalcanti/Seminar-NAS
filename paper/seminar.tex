\documentclass[10pt,        % Don't change the font size!
               a4paper,     % Don't change the paper size!
               journal,     % Journal paper format
%               draft       % Enable this parameter to get a draft version.
               ]{IEEEtran}
\makeatletter

\def\markboth#1#2{\def\leftmark{\@IEEEcompsoconly{\sffamily}\MakeUppercase{\protect#1}}%
\def\rightmark{\@IEEEcompsoconly{\sffamily}\MakeUppercase{\protect#2}}}
\makeatother

% Packages
%\usepackage[latin1]{inputenc}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{xcolor}
% \usepackage[pdftex]{graphicx}
%\usepackage[cmex10]{amsmath}
%\usepackage{algorithmic}
%\usepackage{array}
%\usepackage{mdwmath}
%\usepackage{mdwtab}
%\usepackage{eqparbox}
%\usepackage[tight,footnotesize]{subfigure}
%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}

% \graphicspath{{../pdf/}{../jpeg/}}
% \DeclareGraphicsExtensions{.pdf,.jpeg,.png}

\begin{document}
%
% paper title
\title{Neural Architecture Search and tinyML}
\author{Daniel~Duclos-Cavalcanti}

% The paper headers
\markboth{Seminar for VLSI Entwurfsverfahren, Summer Term 2022}%
{Daniel Duclos-Cavalcanti: Network Architecture Search (NAS)}

% make the title area
\maketitle

%\thanks{Daniel D-C. is with the Department
%of Electrical and Computer Engineering, Technical University of Munich, Bavaria, DE.}% <-this % stops a space

\begin{abstract}
There is no denying the increasing success that Deep Neural Networks (DNNs)
have displayed across various tasks such as image classification, speech recognition, machine translation
and many others. This progress can be attributed largely due to \textit{architecture engineering},
which is a process that requires immense domain expertise, intuition and some amount of trial
and error. This is not ideal as it is both time consuming and error-prone. Neural Architecture Search (NAS)
rises as the next logical step aiming to both automate architecture discovery and further the understanding
of the inner workings of DNNs. This field has grown remarkably within the last 5 years and among the many challenges
of NAS, there are concerns regarding computational costs and time feasability. This however changes within the context
of \textit{tinyML}, which is an expanding field at the intersection of machine learning and embedded systems.
Due to the resource constrained conditions involved in most embedded devices and tinyML
applications, the stages concerning training and performance evaluation are substantially faster in comparison to
the more complex models used in the research of NAS. Thus presenting an interesting opportunity to explore NAS applications
within a different paradigm. Throughout this work, an overview of existing research in NAS, specifically concerned
with the use of evolutionary algorithms and reinforcement learning methods will be presented, as well
as highlighting relevant applicabilities to tinyML.

\end{abstract}

\begin{IEEEkeywords}
Neural Architecture Search (NAS), evolutionary computation (EC), Deep Learning, tinyML.
\end{IEEEkeywords}

\section{Introduction}
The advancement of deep learning, although extremely beneficial, has also caused a continuous demand for
architecture design. This coupled with a growing model complexity still requires ample time and expert knowledge to
continue this progress.

Finally, after the work in \cite{zoph2016neural} proposed by Google, it was shown for the first time that NAS algorithms
have the potential to find models that rival the current state of the art. However, done so in an automated fashion, minimizing
human participation. Since then, many different methods have appeared.

To better visualize NAS, one can categorize it within three dimensions \cite{wistuba2019survey}:
\begin{itemize}
    \item \textbf{Search Space}: defines all the possible architectures that can in principle be considered.

    \item \textbf{Search Strategy}: defines how the \textit{search space} is explored by the algorithm.

    \item \textbf{Performance Estimation Strategy}: defines how performance is evaluated at every architecture iteration.
\end{itemize}

\subsection{Search Space}
NAS is an optimization problem, whose search space is the defining factor to its complexity.
The smaller the search space is, the faster the search may converge,
as well as requiring less computational resources. This comes at the cost of less freedom to
explore unseen architectures and also possibly limiting the complexity of the design.

The simplest approach is to define an architecture as a \textit{chain-structured neural network}, which essentially consists of a
sequence of layers whose inputs are the output of their preceding layer. In this case the space is parametrized by maximum number
of layers, type of operations per layer, and hyperparameters conditioned by the chosen operation.

The next step consists then of including more modern design elements such as skip connections, which has already been seen
in \cite{zoph2016neural} and \cite{pmlr-v70-real17a}. This allows to build more complex \textit{multi-branch networks}, which
cannot be described as simple sequential layer chaining, but as a structure where each layer's input is a function of
previous layer outputs. Therefore, a chain-structured neural network can be seen as an edge-case or simplification of the multi-branch network.
This increases significantly the degrees of freedom of architecture design, which leads to a much larger search space.

Another predominant trend includes the search within \textit{cells} or \textit{blocks}, initially considered by works such as
\cite{zhong2018practical} and \cite{zoph2018learning}. What is proposed, is to break-off architectures in cells, such that
the search space is then designated within a single cell per time. This drastically reduces the search space, as there are
substantially less layers within a cell in comparison to an entire architecture. Additionally, subdivision of architectures
into blocks is considered a good design practice, which also enables easy transferability of blocks to other data sets. The same
modelling used on multi-branch networks can be used with cells, simply replacing layers with cell architectures.

This drives then the discussion between \textit{micro-architectures} versus \textit{macro-architectures}. The macro architecture
attempts to determine how cells should be connected and how many are needed to build a model. On the other hand, the micro
architecture aims to find the optimal structure for each cell. Ideally, both viewpoints should be optimized jointly, which of course
leads to a complex search space. There have been efforts that aimed to minimize this endeavor by fixing macro-architectures with
known working topographies such as in \cite{pmlr-v80-cai18a} with DenseNet \cite{Huang_2017_CVPR}. This practice, dubbed as
\textit{human knowledge injection} attempts to reduce the search space through using domain expertise known to obtain
effective results. This includes human bias in the model.

\subsection{Search Strategy}
As any space search problem, there is a \textit{exploration-exploitation} trade-off to be considered \cite{wistuba2019survey}.
Obtaining a high-performing architecture within a feasible amount of time is desired. However, converting too early to a
suboptimal result is also not the goal.

Based on the current state of the art, NAS search algorithms can be classified mainly intro three different
categories \cite{liu2021survey}:

\subsubsection{\textbf{Reinforcement Learning} (RL)-based NAS Algorithms}
If one considers the development of a neural architecture the agent's action, where the action space is the same as
the search space, it is then possible to frame NAS as a reinforcement learning problem \cite{wistuba2019survey}.
After evaluating the performance of the given trained architecture on fitness data, it is possible to
determine the agent's reward. How this estimation is performed will depend on the given method, more to these
approaches will be seen at \ref{PES}. Furthermore, how the agent's policy is represented and it's optimization will
also vary. More on trade-offs and details to these approaches will be seen in \ref{SOA}.

\subsubsection{\textbf{Gradient}-based NAS Algorithms}
\textcolor{red}{
Consists of transforming the search space from discrete to continuous and performing
gradient descent with respect to the fitness data set.
}

\subsubsection{\textbf{Evolutionary Computation} (EC)-based NAS Algorithms}
By the application of well established EC methods, which are various techniques based on the evolution of
species within nature, many different efforts were implemented to navigate their respective search space.
\textcolor{red}{
These varied from genetic algorithms (GAs), genetic programming (GP) up until particle swarm optimization (PSO)
techniques.
}


\subsection{Performance Estimation Strategy}
\label{PES}
Independent of the search strategy, it is necessary to know how any given architecture performed in order
to guide the next steps of the algorithm. There are many ways to estimate said performance, whereas the simplest
form would be complete training and validation. Given the complexity and size of the search spaces within NAS, this
requires GPU days in the order of thousands \cite{wistuba2019survey}. This is why extensive research has been
employed to reduce time on performance estimation, since it is by far the greatest bottleneck.
Some of the approaches such as
\textcolor{red}{
lower fidelity techniques, learning curve extrapolation, weight inheritance and one-shot models were utilized
to ensure achievable periods of search.
}

\section{State of the Art}
\label{SOA}

\subsection{Overview}
Currently RL-based algorithms are extremely costly in terms of computation, requiring thousands of
graphics processing cards (GPUs) for days. This is already the case for median-scale data sets, as
as data grows more complex, so does the processing time needed to search for a suitable architecture.

Gradient-based algorithms are however faster, there are cases such as DARTS, where processing power
is cut down to single digit GPUs \cite{liu2018darts}. However, as many gradient-based approaches they have
to adapt the problem to be able apply their algorithm. This often causes ill-conditioned architectures due to
the improper relationship brought by this adaptation \cite{liu2021survey}.

On the other hand, EC methods, while not perfect, have been around for decades and are easily applicable
to solve complex non-convex optimization problems, as they are insensitive to local minima and do not require
gradient information \cite{liu2021survey}.

\subsection{Evolutionary Computation-based Efforts}

\subsection{tinyML and NAS}

\section{Conclusion and Future Work}

% references section
\newpage
\bibliographystyle{IEEEtran}
\bibliography{refs}

%
\end{document}
