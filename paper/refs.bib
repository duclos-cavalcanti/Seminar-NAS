@article{wistuba2019survey,
  title={A survey on neural architecture search},
  author={Wistuba, Martin and Rawat, Ambrish and Pedapati, Tejaswini},
  journal={arXiv preprint arXiv:1905.01392},
  year={2019}
}
@article{liu2021survey,
  title={A survey on evolutionary neural architecture search},
  author={Liu, Yuqiao and Sun, Yanan and Xue, Bing and Zhang, Mengjie and Yen, Gary G and Tan, Kay Chen},
  journal={IEEE transactions on neural networks and learning systems},
  year={2021},
  publisher={IEEE}
}
@article{zoph2016neural,
  title={Neural architecture search with reinforcement learning},
  author={Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1611.01578},
  year={2016}
}
@inproceedings{cai2018efficient,
  title={Efficient architecture search by network transformation},
  author={Cai, Han and Chen, Tianyao and Zhang, Weinan and Yu, Yong and Wang, Jun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}
@InProceedings{pmlr-v70-real17a,
  title = 	 {Large-Scale Evolution of Image Classifiers},
  author =       {Esteban Real and Sherry Moore and Andrew Selle and Saurabh Saxena and Yutaka Leon Suematsu and Jie Tan and Quoc V. Le and Alexey Kurakin},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2902--2911},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/real17a/real17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/real17a.html},
  abstract = 	 {Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Our goal is to minimize human participation, so we employ evolutionary algorithms to discover such networks automatically. Despite significant computational requirements, we show that it is now possible to evolve models with accuracies within the range of those published in the last year. Specifically, we employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions and reaching accuracies of 94.6\% (95.6\% for ensemble) and 77.0\%, respectively. To do this, we use novel and intuitive mutation operators that navigate large search spaces; we stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements.}
}
@inproceedings{zhong2018practical,
  title={Practical block-wise neural network architecture generation},
  author={Zhong, Zhao and Yan, Junjie and Wu, Wei and Shao, Jing and Liu, Cheng-Lin},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2423--2432},
  year={2018}
}
@inproceedings{zoph2018learning,
  title={Learning transferable architectures for scalable image recognition},
  author={Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={8697--8710},
  year={2018}
}
